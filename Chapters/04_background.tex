\chapter{Background}

\section{Literature Review}
TODO THIS I SHOULD DO SOMEHOW? HOW DO OTHERS DO IT?


\section{Machine Learning}
Machine learning is an area of research concerned with learning to perform some task based on some data. Machine learning is generally very broad, going from TODO to TODO.

A subset of machine learning are supervised learning methods. These methods
split their data into \emph{features} and \emph{targets}, and attempt to
predict the targets based on the features. We know the supervised learning
problems as regression and classification problems, depending on whether we're
predicting a continuous value (regression) or a discrete category
(classification).

In supervised learning, we often have our data in tabular form, meaning that
the data is structured in a table of rows and columns. Rows represent
observations and columns represent features. The regression problem is a
problem of finding the best description of the relation between target values
and features. To perform regression, every row in the tabular data is
associated with a target value, and a certain model for the relation between
the features and the target value is chosen. This model has some parameters
that can be chosen during the regression problem, known as "fitting" the
parameters. The parameters are chosen to minimize some loss function. The loss
function is a mathematical function that represents the "distance" between the
true target value and the predicted target value. The general setup for tabular
regression can be seen in figure TODO:FIGURE. TODO: EXPLAIN OUT-OF-SAMPLE PREDICTION

As an example of a regression problem, we have the linear regression problem.
In linear regression, our relation between features and models are represented
as a linear function. We assume that we have our features in a matrix $X$,
where row $t$ of the matrix is known as $x_t$. The true target associated with
row $t$ is known as $y_t$. 

\begin{equation}
  
\end{equation}

associating every row with a target value, choosing a model for the
relation between the 

TODO: MACHINE LEARNING IS BASED ON OPTIMIZATION

TODO: FIGURE SHOWING MACHINE LEARNING CONCEPT: CLOUD OF DATA -> PREDICITONS

W

TODO: ML -> REGRESSION & CLASSIFICATION PROBLEMS 

TODO 

% TODO: Online machine learning is a field of machine learning of methods that train on
% their data sequentially. This can be done for computational reasons, but also in
% cases where new data becomes available as time passes. Online machine learning
% methods are also useful when predicting a system that changes over time, as the
% methods can adapt to fit changing distributions.

\subsection{Missing Data}

In machine learning in general, data might go missing. There might be a sensor
that isn't working, or a server might be down, or any number of other
situations. To continue using a machine learning method, you have to make a
decision of what to do about this missing data, which is known as imputation.

Data can be missing for different reasons, with different implications. Missing
data mechanisms area classified into three categories TODO:REF:

\begin{enumerate}
  \item MCAR - Missing Completely At Random
  \item MAR  - Missing At Random
  \item MNAR - Missing Not At Random
\end{enumerate}

MCAR data is data that is missing independent of the data itself, so the data
loss mechanism has no relation to the data. MAR data is missing dependent on
the rest of the data, but independent of the missing data. This could e.g. be
seen in a survey where male responders might be less likely to report their
income. MNAR data is missing dependent on the missing data. This could for
example be in a system where some values result in errors. Maybe the survey
software doesn't support heights greater than 2m, so these have all gone
missing.

TODO: WHEN SHOULD I FIRST DISCUSS MISSING DATA IN THE TRAINING SET VS TESTING
SET?

When using data for machine learning methods, we are faced with a choice of
what to do with missing data. Data can be missing both in-sample and
out-of-sample. Whenever data is missing, we have the choice to either replace
it with some value, known as imputation, or discarding it. Discarding data
out-of-sample means being unable to produce a prediction, while imputing data
in-sample can skew the training distribution and occasionally affect model
performance.

The decisions and outcomes available regarding imputation are shown in table
\ref{tab:imputation}.

\begin{table}
  \begin{tabular}{c|cc}
    & In-sample & Out-of-sample \\
    \hline
    Impute & Skew training distribution & Produce subpar prediction \\
    Discard & Do not learn from observation & Produce no prediction
  \end{tabular}
  \caption{Decisions available when data is missing in the in-sample and
  out-of-sample case.}
  \label{tab:imputation}
\end{table}

Missing data can also confer extra information when it is MNAR. Some models
natively support missing data, and generally expect for data to be either MAR
or MNAR and contain extra information, e.g. TODO:REF CATBOOST.


Imputation methods include:
\begin{enumerate}
  \item Mean/Mode Imputation
  \item Using Domain knowledge
  \item Linear reconstruction
  \item Tabular Regression Methods
  \item Deep Learning Methods
  \item Fully Conditional Specification
\end{enumerate}
and more.

1) Mean/mode imputation replaces the missing value with the historical mean or
   mode of the feature. This is simple and practical, but might considerably
   skew the distribution. Nonlinear regression methods can significantly
   deteriorate if this imputation is used in the training set due to changing
   in the distribution TODO: REF

2) Sometimes, domain knowledge can be used to reconstruct missing data. This is
   often in situations with either MAR or MNAR data. In our situation from
   before where every height greater than 2m was missing, someone with
   knowledge of the situation could reconstruct the missing data very well by
   e.g. assigning every missing height to just over 2m. TODO: MAYBE GIVE THE IE
   IMBALANCE PRICE THING TOO?
    
3) Linear reconstruction uses a linear mapping to reconstruct features. This
   linear mapping can be learned in several ways. Examples of linear
   reconstructive methods will be seen later TODO. Linear reconstruction is
   really a special case of the next set of methods, which is

4) Using some tabular regression method, e.g. KNN or Random Forest TODO:REF,
   for reconstruction. These methods often do not support missing data in their
   own training set, so some method of imputation or data removal is still
   needed.

5) Deep learning methods have also been developed to fill out missing data,
   e.g. Imputational Autoencoders TODO:REF and Generative Adversarial Networks
   TODO:REF. 

6) Fully conditional specification is elaborated in the section TODO

TODO: MAYBE SOMETHING ON LIKE PROBABILITIES FOR THE LINEAR RECONSTRUCTION SHIT.

\subsubsection{Fully Conditional Specification}
Fully Conditional Specification is a method for imputing missing features.
TODO

\subsection{Evolving Features}

When operating with evolving feature spaces, we can split the feature dynamics
into four categories. A feature can at any time step either:

\begin{enumerate}
  \item Appear for the first time
  \item Be observed regularly, having been observed before
  \item Be randomly missing for one or more time steps
  \item Permanently disappear, never reappearing
\end{enumerate}
TODO


\subsection{Conditional Expectation}
TODO: CONDITIONING AND MARGINALIZATION

\subsection{Utilitarian Online Learning}
%TODO: THERE'S NO REASON TO SPEND SO MUCH TIME ON THIS WHEN I'M GOING TO SAY THAT IT LITERALLY DOES NOT WORK

Utilitarian Online Learning mechanisms are online machine learning models that
support arbitrarily evolving feature spaces. They are designed for streaming
situations with arbitrarily evolving feature streams. 

Current methods are generally all classification methods. He et al. (2023)
TODO:REF classifies these methods into the following three groups

\begin{enumerate}
    \item Passive-Aggressive
    \item Feature Correlation
    \item Evolutionary Ensemble
\end{enumerate}

Passive-Aggressive methods are methods that wait to update their weights until
their prediction quality deteriorates considerably. This means that when new
features appear, they do not use them in their predictions until their previous
features do not provide enough information anymore. TODO: EXAMPLES?

Feature Correlation methods are focused mainly on how to deal with features
that were previously present and then go missing. They attempt to reconstruct
missing information from inter-feature correlations. Their method is to attempt
to find a mapping from the observed features at a specific time, also known as
the instance feature space, into the space of all features observed up until
that time, known as the universal feature space. They then use this
reconstruction for the final prediction. TODO: EXAMPLES?

Evolutionary Ensemble methods work by constructing ensembles of weak learners
that develop over time. The state-of-the-art is generally tree-based models.
One example is the TODO: THE ONE THAT WORKS BY CREATING STUBS

He et al. (2023) TODO: REF covers six methods. They are all classification
methods. Their performance is tested on four datasets, and they are ranked
accordingly. TODO: RANKING PAPERS TODO: MORE HERE TODO: SOME OF THE METHODS
PAPERS

\subsubsection{OCDS}

TODO:REF introduces a utilitarian online learning model known as OCDS (Online
learning for Capricious Data Streams). In the classification of TODO, it falls
into the feature correlation category. At its base, it is a linear classifier
and a linear feature reconstruction method for imputing missing features both
in-sample and out-of-sample.

TODO: WRITE ABOUT $u_t$ AND $x_t$ AND STUFF. TODO: MOVE THE NOTATION UP TO OCO.

The linear classifier has weights $w$. For every time instance, an observation
$x_t$ is received with a set $d_t$ of present features. The missing features
are reconstructed using a linear mapping $\psi : X_t \rightarrow U_t$. The
linear mapping is given by 

\begin{equation}
  \psi(x)_i = \frac {1}{|d_t|}\sum_{k=1}^{|d_t|} \mathbb E \lbrack x_i | x_k \rbrack
\end{equation}

Assuming a multivariate Gaussian distribution between $x_i$ and $x_k$, so that
$\mathbb E \lbrack x_i | x_k \rbrack = \alpha_{i,k} x_k$ for some coefficient
$\alpha_{i,k}$.

The coefficients are stored in a matrix G, so that $G_{k,i}=\alpha_{i,k}$, or $G_{k,i} x_k = \mathbb E \lbrack x_i | x_k \rbrack$.

TODO: TRANSPOSITION OF $GR_t$

The subset of G that is relevant for time step $t$ is named $Gr_t$,
\begin{equation}
  Gr_t = G_{d_t, u_t}
\end{equation}

It is the rows of $G$ for every present feature in the set $d_t$. The
reconstructive mapping is then given by 

\begin{equation}
  \psi(x_t) = \frac {1} {d_t} Gr_t x_t
\end{equation}

The method then generates a prediction based on this. It reconstructs the
missing features in $\tilde x$, $\tilde x = \psi(x_t)_{TODO: UNOBSERVED
FEATURES}$. The reconstructed and the observed features are used for an
ensemble prediction weighed by $p$.

\begin{equation}
  \hat y_t = p (\bar w^T x_t) + (1-p) (\tilde w^T \tilde x)
\end{equation}

The final predicted class is then given by the sign of $\hat y$.

To update the weights $w$ and the matrix $G$, the paper sets up the objective
function $\mathcal F$. They add a supervised loss term and reconstructive loss
term, as well as two regularization terms on the coefficients $w$. 

The OCDS algorithm uses MSE loss as its supervised loss. The supervised loss
term is given in terms of the reconstructed features by 

\begin{equation}
  (w^T \frac {1}{|d_t|} Gr_t x_t - y_t)^2
\end{equation}

The reconstruction loss term is given by 
\begin{equation}
  \|(\Pi_{d_t} Gr_t x_t) - x_t\|^2_2
\end{equation}
where $\Pi_{d_t} : U_t -> X_t$ is the projection operator TODO.

There is then an L1 regularization term on the coefficients
\begin{equation}
  \|w\|_1
\end{equation}

and a regularization term to ensure close coefficients for highly dependent
features.
\begin{equation}
  quTr(w_t^T L w_t)
\end{equation}

where L is the graph laplacian of G, and Tr is the trace operator.

These terms are added together with coefficients $\alpha$ for the
reconstruction loss, $\beta_1$ for the $L1$ regularization, and $\beta_2$ for
the graph regularization.
\begin{equation}
  \underset{w,G}{\text{argmin} \,\, \mathcal F} = (w^T \frac {1}{|d_t|} Gr_t x_t -
  y_t)^2 + \alpha \|(\Pi_{d_t} Gr_t x_t) - x_t\|^2_2 + \beta_1 \|w\|_1 +
  \beta_2 Tr(w_t^T L w_t)
\end{equation}

This is a biconvex problem in $G$ and $w$, so using coordinate gradient
descent, this is guaranteed to converge with a proper choice of step size
$\tau$.  The partial derivatives are given by

\begin{equation}
  \nabla_w \mathcal F = -2(\hat y_t - y_t) + \beta_1 \partial \|w\|_1 +
\beta_2(L + L^T)w) 
\end{equation}

\begin{equation}
  \nabla_G \mathcal F = TODO
\end{equation}

The update steps are then

\begin{equation}
  w := w - \tau \nabla_w \mathcal F \,\,\,\, G := G - \tau \nabla_G \mathcal F
\end{equation}

The matrix G can equivalently be updated only for the subset $Gr_t$ for
computational efficiency.


TODO: EVERYTHING RELATED TO BCE AND LOGISTIC FUNCTION SHOULD GO IN THE "NEW STUFF" PART

I introduce a modification to the OCDS algorithm based on the binary
cross-entropy loss function, since this is a better loss function for
classification tasks TODO:REF. This modification requires a few changes. The
classes are now $0$ for the negative class and $1$ for the positive, instead of
$-1$ and $1$. $\hat y$ is given instead by

\begin{equation}
  \hat y_t =  h(w^T \frac {1}{|d_t|} Gr_t x_t)
\end{equation}

Where h is the TODO logistic function, $h(a) = \frac{1}{1+e^{-a}}$. The
predicted class is now the positive class when $\hat y>0.5$, and the negative
class when $\hat y \leq 0.5$.

The supervised loss term now becomes
\begin{equation}
  y_t log(\hat y_t) + (1-y_t) log (1-\hat y_t)
\end{equation}

and the terms in the partial derivatives become instead

\begin{equation}
  \nabla_w \mathcal F = -(\hat y_t - y_t) + \text{regularization} \,\, \nabla_G
  \mathcal F = TODO + \text{reconstruction error}
\end{equation}

TODO: EVERYTHING FROM HERE TO GAME THEORY SHOULD GO SOMEWHERE ELSE



% He et al. (2019) TODO: REFERENCE introduces a Utilitarian online learning model
% known as the OCDS algorithm. It is a feature correlation method. It creates and
% evolves a linear classifier with weights $w$. It then also creates a linear
% mapping from the observed feature space $X_t$ to the universal feature space
% $U_t$ named $\psi : X_t \rightarrow U_t$. The linear mapping is understood as a
% generative graph, where every vertex in the graph is a feature in the universal
% feature space. The vertices then have out-edges to every other vertex that they
% have appeared in concert with, and the weight of these out-edges represent the
% feature relatedness. %TODO: EXPLAIN
% %BETTER
% %
% The paper defines a vertex approximator $\Phi_i$, which is a vector that for
% feature $i$ holds every out-edge in the graph. The matrix $G \in
% \mathbb{R}^{|U_t| \times |U_t|}$ holds the vertex approximators for every
% feature as rows so that the reconstructed features are given by $\psi(x_t)=
% \sum_{i \in X_t} \Phi_i x_{i} $. The prediction then uses the observed features
% where available and reconstructed features where not, and uses these two
% classifiers in an ensemble. The weights are split in two, $\bar w_t$ and
% $\tilde w_t$, where $\bar w_t$ refers to the weights for the observed instance
% features $x_t$, and $\tilde w_t$ refers to the weights for the unobserved
% instance features. $\tilde x_t$ refers to the reconstruction of the features in
% the universal feature space not observed at time $t$, and $p$ is a weight for
% the ensemble, determining the impact of the observed features and the
% unobserved features. TODOTODO \begin{equation} \hat y= p \langle \bar w_t,
% x_t\rangle + (1-p) \langle \tilde w_t, \tilde x_t \rangle \end{equation} The
% predicted class is then given by the sign of $\hat y$, and $p$ is updated. $w$
% and $G$ are updated by gradient descent, and the problem is biconvex, so
% convergence is guaranteed with a proper choice of step size. %TODO: THERE
% %SHOULD MAYBE BE EVEN MORE OCDS TODO: FIGURE OUT WHAT TO DO RELATING TO LOSS
% %FUNCTIONS AND SIGNS AND STUFF


\section{Game Theory}

%TODO: MAKE IT CLEAR THAT WE'RE NOT FOCUSING ON DESIGNING AN AUCTION, MORE ON THE SELLER SIDE
%
%TODO: I NEED TO FIND A NEW NAME INSTEAD OF AUCTIONS, MAYBE JUST GAMES

Section \ref{sec:auctions} to \ref{sec:auction design} are generally based on \cite{algo_game_theory}.

\subsection{Auctions} \label{sec:auctions}
An auction mechanism is a method for distributing goods and money between one
or more buyer(s) and one or more seller(s). A common example of an auction is a
second-price auction. Every bidder secretly submits what they would be willing
to pay for the good being sold, and the person with the highest bid will pay
the second-highest bid for the good. This is an auction that is commonly
studied, since it has several nice properties.

The study of auctions is a part of game theory. The goal is to investigate the
incentives and dynamics of different auction mechanisms. Theoretical knowledge
of auction mechanisms can be used by market creators to inform their choice of
auction mechanism, so that the market can be made with the preferred
properties. Game theory centrally investigates what is known as the Nash
equilibrium. A Nash equilibrium in a game theoretical game is one where every
agent knows the equilibrium strategy of all other agents, and no agent can
improve their own position by changing their own strategy while every other
agent's strategy stays fixed. The bids of buyers and provided information from
sellers is generally known as the strategy of the agent. It reflects some
internal information that the agent has, known as the type of the agent.

Designing an auction mechanism is generally posed as a constrained optimization
problem. The most common measures to maximize for are social welfare and total
revenue.

Social welfare is a measure of how much value every participant in an auction
receives from participating. Every agent $i$ has social welfare $U_i(x) - p_i$,
where $x$ is the allocation of goods and $p_i$ is the amount of money paid by
agent $i$. This equation holds for both bidders and sellers, but in the
seller's case, the value $p_i$ will be negative, since they receive money. The
sum social welfare is then given by $\sum_i^N U_i(x)-p_i$. The terms $p_i$
cancel out, since the total amount paid by buyers equals the total amount
received by sellers, so the social welfare optimization becomes simply
\begin{equation}
    \text{max} \sum_i^N U_i(x)
\end{equation}

Total revenue is the amount paid by all bidders, or the amount earned by all
sellers. Maximizing total revenue is sensible for market designers since
sellers would prefer to maximize their revenue, so sellers would be strongly
incentivized to provide their goods in this case. Also TODO: THIS PARAGRAPH.


Auctions can also exist where the agents do not have full information before
the auction runs. This can be for several reasons. Sometimes it is due to some
inherent randomness in the auction mechanism, like a lottery allocation system,
and sometiems it is due to an underlying factor that agents do not know. Both
of these situations change the auction setting in the same way introducing a
probabilistic component. These are known as Bayesian games, and agents in
Bayesian games optimize for their expected return from participating in the
auction, instead of their deterministically determined return. An equilibrium
is known as a Bayesian-Nash Equilibrium, which is the set of strategies where
no agent can improve their expected return by changing their strategy keeping
all other agents' strategies fixed. In constructing a Bayesian auction
mechanism, we instead maximize for expected social welfare or expected total
revenue.

Within auction design, there are two properties that come up very often:
\begin{enumerate}

    \item Truthfulness - In the equilibrium state of the auction,
          do participants truthfully report their information to the market?

    \item Ex-Post Individual Rationality (EPIR) - In the equilibrium state of
          the auction, no actor is worse off by engaging in the auction than not
          engaging. This is a property relevant to Bayesian auctions, since these
          auctions can have positive expected value for an agent without
          guaranteeing positive returns.

\end{enumerate}

%TODO: WE DON'T DO EPIR, AND TRUTHFULNESS ONLY HOLDS IN EXPECTATION.
%TODO: TRUTHFULNESS + IR SHOULD BE REWRITTEN TO COUNT FOR GENERAL GAMES, NOT ONLY AUCTIONS

\subsection{Truthfulness} Why are we interested in truthfulness? Sometimes,
auction mechanisms can lead to what is known as strategic bidding. This means
that the agents in the auction get better outcomes by not giving their true
information, even if all other actors give their true information. A good
example is the second price auction mentioned earlier, if we assume that the
good has one true value. If bidders don't know the exact value of the good
they're buying, and instead only have an estimate, then an effect occurs that
is known as the "Winner's Curse"
%TODO: REF FOR WINNER'S CURSE 

We can show the winner's curse e.g. by investigating an auction setting where
there is a single good for sale. Every $i$ has an estimated valuation $v_i$
given by $v_i = v + \epsilon_i$, where $v$ is the true value of the good and
$\epsilon_i$ is iid. centered finite variance gaussian noise, $\epsilon_i
    \overset{iid}{\sim} \mathcal{N}(0,\sigma^2)$. If every bidder bid their own
estimate of the value, which is their best guess for the value and therefore
their truthful bid, then the winner will end up paying the second-highest bid
present. The expected value for the second-highest present bid is given by
$\mathbf{E}[\text{second highest}(\{v+\mathcal{N}(0,\sigma^2)\}_{i=1}^N)]$,
which is greater than the true value of the good already at $N=4$ bidders,
meaning at $N\geq4$, the auction winner will pay more than the true value of
the good if every bidder bids their best estimate of the value of the good.

%TODO: SECOND HIGHEST NOISE>0 SHOULD MAYBE BE PROVED BUT ITS ALSO OBVIOUS?

Bidding optimally in a non-truthful auction can be very difficult for bidders.
This means that the auction often won't operate optimally, since the bidders
won't be able to find equilibrium, and the properties of the auction in
equilibrium will be lost. If the auction was constructed to maximize for some
value in equilibrium, then it also won't maximize for this value any longer,
since it won't reach equilibrium. In a truthful auction, bidders won't gain
anything from strategic bidding, and will instead simply bid with the best
information they have.

\subsection{Ex-post Individual Rationality (EPIR)}
Why are we interested in ex-post individual rationality? An auction with a
random component, known as a bayesian game, might have an expected positive
return for an agent without guaranteering a positive return. This could e.g. be
a lottery setup: You can buy a lottery ticket for 1€ and you have a 0.0015\%
chance to win 100,000€. This has an expected return of 0.5€, but there is no
guarantee that any individual agent has a positive return from participating in
the auction, so it is not ex-post individually rational. The vast majority of
agents will actually not win from engaging, so due to risk aversion many agents
will rather sit out than engage. In an ex-post individually rational auction,
no agent will have negative return from engaging in an auction, so no agent
will sit out due to risk aversion.

\subsection{Mechanism Design} \label{sec:auction design}
%TODO: THIS SECTION SHOULD PROBABLY BE REMOVED/RENAMED/REWRITTEN, MORE LIKE "GAME DESIGN"
%Game Design? Auction design? Mechanism design?

Generally, when looking for a good mechanism for a market situation,
one is looking for a truthful, ex-post individually rational mechanism that
maximizes either social welfare or revenue. For non-Bayesian settings,
meaning settings where every agent knows their exact valuation of any
allocation of goods ahead of time, there exists always a truthful social
welfare-maximizing mechanism, as well as a method for constructing it, known as
the Vickrey-Clarke-Groves (VCG) mechanism, as demonstrated in
\cite{algo_game_theory}, chapter 7. For Bayesian settings, there is no
general method for constructing the social welfare-maximizing mechanism, and no
general mechanism exists for revenue maximization settings either.

TODO ALL OF THIS IS VERY TECHNICAL, IT NEEDS TO BE EASED INTO MORE

TODO The example below is weird:

As mentioned before, the second-price auction is a very well known example of
a TODO?. Imagine the non-Bayesian setting with a single sold good
where every agent $i$ has a private valuation of the good $v_i$. This could
e.g. be an auction with a single collector's item that a group of collectors
are bidding on simple for the joy of owning the item. The second-price auction
is the VCG mechanism in this situation, meaning that it is the truthful
mechanism maximizing social welfare. The good will be allocated to the agent
$i$ with the highest valuation $v_i$, and the total social welfare produced by
the auction will be

\begin{equation}
    U = \underset{i}{\max} \, v_i - \underset{i}{\text{second highest}} \, v_i
\end{equation}

This happens to also be the revenue maximizing mechanism in this setting, since
the setting is fairly simple, and a number of other well-known mechanisms are
also revenue- and social welfare-maximizing in this setting, hereunder most
generally used single-good auctions. %TODO: IS THIS A GOOD WAY TO WRITE IT?

%TODO: DO I WANT TO JUST SHOW THE MATHEMATICS FOR THE SECOND PRICE AUCITON?


%TODO: DATA AUCTIONS FROM MARIANNA RUI

Data is a complicated good when it comes to auction design, since the owner of
the data still owns their data after selling it to a buyer. Also, selling data
as-is gives away the value of the data, as the buyer can turn around and sell
it on to the next person. Therefore, data auctions need to operate a little
differently. The paper TODO: REF TOWARDS DATA AUCTIONS constructs welfare
maximizing and revenue maximizing auctions in settings with externalities
between participants. The focus is to TODO

\subsection{Shapley Values}

This section is based on the book chapter
\cite{cooperative_game_theory_toru_hokari}. Within data markets, a concept
known as Shapley values often comes up. Shapley values come from cooperative
game theory. Cooperative game theory deals with games where agents can
externally enforce cooperation. This could be through methods like a contract
or similar situations. Then, the agents in a cooperative game are working
together instead of against each other. In cooperative game theory, groups of
agents are known as coalitions, and the coalition of all agents is known as the
grand coalition.

A game in cooperative game theory has a specific payout or cost for every set
of agents. An example of a game could be a shared taxi trip between three
people, where the people are the agents. The taxi trip will have a different
cost depending on which people go on the taxi. Then, the game has a
characteristic function $v(S)$ that maps from any set of agents $S$, known as a
coalition, to a payout or cost, $v: 2^{|N|} \rightarrow \mathbb R$. Here, $|N|$
is the total number of agents.

TODO: BE CLEAR WITH THIS EXAMPLE, MAYBE MAKE A DIAGRAM?

Let's think again about our taxi trip, and let's say three people are looking
to share a taxi. We want to know a fair way to split the cost of this taxi trip
between the three participants. Economist Lloyd Shapley has developed four
properties that a fair distribution of cost or payout should fulfill: %TODO
% :THIS IS VERY BLOGGY AND NOT VERY THESIS-Y

\begin{enumerate}

    \item Balance: All payout should be distributed between the agents

    \item Symmetry: If two players have the same effect in all coalitions, then
          they should receive the same portion of the total payout

    \item Zero Element: If a player has no effect in any coalition on the
          payout, then they shouldn't receive anything

    \item Additivity: If you add two games together, the payouts should also
          add together. %TODO: THESE COULD BE BETTER

\end{enumerate}

There turns out to be only one unique way of distributing payout in this way,
which is known as the Shapley value. Every agent in the game receives a Shapley
value that represents their part of the payout. The Shapley value $\psi_i$ for
agent $i$ can be computed by investigating the effect of adding an agent to
different coalitions. So, let's take a coalition $S$ that does not contain the
feature $i$. We note the difference between the payout of the coalition $S$,
$v(S)$, and the payout of the coalition $S$ with feature $i$, $v(S \Cup
\{i\})$. We'll call this difference $\Delta v_{S,i} = v(S \Cup \{i\}) - v(S)$.
When calculating the Shapley value, we need to investigate every order of
adding features to produce the grand coalition. If three features were present,
features $1,2,3$, then we would have the following 6 ways to add in all
features: $1,2,3,$, $1,3,2$, $2,1,3$, $2,3,1$, $3,1,2$, $3,2,1$. The Shapley
value of feature $1$ would then be the average of $\Delta v_{S,i}$ for every
ordered set of features encountered before $i$. Mathematically, it is expressed
as:

\begin{equation}
    \psi_i = \sum_{S \subset N \\ \{i\}} \frac{|S|! (|N|-|S|-1)!}{|N|!} (v(S \Cup \{i\} - v(S)))
\end{equation}

The fraction $\frac{|S|! (|N|-|S|-1)!}{|N|!}$ expresses the number of times
every coalition $S$ appears before feature $i$ in the set of all orderings of
all features.

TODO: CREATE AN EXAMPLE FOR SHAPLEY VALUES TO MAKE IT EASIER TO UNDERSTAND

%TODO: I MADE THIS SUPER CONFUSING
%TODO: I CALCULATED SHAPLEY WRONG BEFORE

%TODO: SHAPLEY VALUES AND MARKETS PAPER: TRUE TO THE MODEL OR TRUE TO THE DATA?


\subsection{Analytics Markets}
Analytics markets are markets where sellers sell features and buyers buy
predictions for a certain prediction task, and the interaction is mediated by a
market with an inbuilt prediction method.


Analytics markets are markets designed for selling features and buying
prediction performance. A market participant could e.g. be a wind producer who
wants to predict their generation capacity, and sellers could e.g. be a weather
forecasting firm.  In a regression market, the buyer would then specify a
regression problem, here predicting generation capacity, and the market would
allocate features and distribute payment based on the performance of the model
trained on the sold data. Exchanging data in this way gives several benefits:

\begin{enumerate}
    \item The actual data is never given away, meaning the buyer cannot go out and resell the data elsewhere.
    \item The buyer only pays for data that is actually valuable to them.
    \item The sellers are paid directly for the quality and value of their data.
\end{enumerate}

An analytics market is one where we have two classes of agents:

\begin{enumerate}
    \item Buyers, who enter the market with a prediction task and some of
      information about what they're willing to pay for prediction quality.
    \item Sellers, who enter the market with some number of features for sale.
\end{enumerate}

%TODO: WHAT ABOUT MARKETS FOR BUYING SAMPLES INSTEAD OF FEATURES?
Market mechanisms must then establish a way to price data and assign it to a
buyer's regression task, as well as a way to distribute payment between the
sellers. Auction mechanisms for several specific market situations have been
constructed. Agarwahl et al. (2023) %TODO: REF TOWARDS DATA AUCTIONS

constructs an auction mechanism for a single, monopolistic seller where buyers
face negative externalities from seeing data assigned to other auction
participants, as can be expected if buyers are in competition with each other.
%TODO:
%MORE EXAMPLES OF FORMING AUCTIONS

Agarwahl et al. (2019) %TODO: REF

lays out a mathematical model for a regression market without externalities.
The paper does not go into the specifics of the regression method, but instead
focuses on modeling the market. The investigated model is a market with
several sellers and buyers. Each seller provides a number of features, and all
features are used together to perform a regression for each buyer. Each buyer
provides a regression task and a monotonically increasing utility function
representing how much they're willing to pay for a certain performance in their
regression task. The paper then defines along with the properties of the
Shapley value payment a fifth propoerty for fair payout in a regression market,
which it calls robustness to replication. A market is robust to replication if
no seller can increase their total revenue by replicating their features. The
paper constructs a mechanism for establishing a revenue maximizing pricing
mechanism for buyers using the multiplicative weights method. The general
market setup from this paper is used in most regression markets created since.
The paper reaches a central conclusion: Under the given market model, there
does not exist a Shapley-fair way to distribute payment between the features of
the market participants that is also robust to replication under the
observational shapley value. 

%TODO: THOSE THINGS NEED TO BE EXPLAINED AHEAD OF
%TIME.

Falconer et al. (2024) further discusses robustness to replication in analytics markets.
%TODO: TOWARDS REPLICATION-ROBUST ANALYTICS MARKETS TODO: I CAN'T REFERENCE IT YET SINCE IT'S UNDER REVIEW

Pinson et al. (2022) %TODO: REF
goes through applications of regression markets within energy forecasting, laying out methods for distributing payment
between features when using both batch regression and online regression in the style of Agarwahl et al. (2019).
Falconer et al. (2023) %TODO: REF
creates a market based on bayesian linear regression and determines how to distribute payment when the output is a
probability distribution instead of a single value prediction. %TODO: IS IT BAYESIAN LINEAR REGRESSION?

%TODO: MORE KIND OF "USE CASES"


% TODO: I PROBABLY WON'T DO BAYESIAN, ACTUALLY
% \section{Bayesian Machine Learning}
% Bayesian machine learning is a subsection of machine learning where model parameters are modeled as probability
% densities instead of simple values. Priors are given for the model parameters and probability densities are then
% updated based on the training data using Bayes's theorem, known as \textit{conditioning} the parameters. Bayes's theorem
% is given as follows: %TODO: REFER TO BAYESIAN DATA ANALYTICS BOOK
% \begin{equation}
%     P(A|B) = \frac{P(B | A) P(A)}{P(B)}
% \end{equation}
% A and B are here events, and P is the probability for an event. The $|$ means conditioned on. TODO
% TODO: MACHINE LEARNING. $\theta$ represents our parameters, $y$ represents TODOASDFPOAJEWSRKASDFJASKLDFJ

% %TODO: BAYES'S THEOREM



% An example of a bayesian machine learning model is bayesian linear regression. Here, we perform linear regression just
% as we know it, but where our parameters are probability densities. This gives us a probability density as an output as
% well. The simplest method of bayesian linear regression uses gaussian priors for the parameters. %TODO: CONJUGATE PRIORS


